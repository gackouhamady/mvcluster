## ðŸ“¦ Graph Clustering Architectures: GCC and LMGEC
This README provides a detailed architectural overview for two unsupervised graph clustering methods implemented in this repository:

- GCC â€“ Graph Convolutional Clustering (single-view)

- LMGEC â€“ Linear Multi-view Graph Embedding and Clustering (multi-view)

The pipeline stages for each model are presented as text-based flow diagrams to enhance clarity, reproducibility, and implementation alignment.

## ðŸ§  GCC â€” Graph Convolutional Clustering

``` text 
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚    INPUT GRAPH DATA         â”‚
                     â”‚  A âˆˆ â„â¿Ë£â¿: adjacency matrix â”‚
                     â”‚  X âˆˆ â„â¿Ë£áµˆ: node features     â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ GRAPH PROPAGATION (SGC-like)        â”‚
                  â”‚ T = D_Tâ»Â¹ (I + SÌƒ), Táµ–X â† T^p X       â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ LINEAR EMBEDDING                     â”‚
                â”‚ Z = Táµ–X W,  W âˆˆ â„áµˆË£á¶                   â”‚
                â”‚ Orthogonal constraint: Wáµ—W = I       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ CLUSTERING                                   â”‚
         â”‚ G âˆˆ {0,1}â¿Ë£áµ: assignment matrix              â”‚
         â”‚ F âˆˆ â„áµË£á¶ : cluster centroids in embedding     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ JOINT OBJECTIVE FUNCTION                             â”‚
       â”‚ L = â€–Táµ–X - Táµ–XWWáµ—â€–Â² + â€–Táµ–XW - GFâ€–Â²                    â”‚
       â”‚  â†’ Equivalent to: â€–Táµ–X - GFWáµ—â€–Â²                      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ OPTIMIZATION (ALTERNATING MINIMIZATION)                â”‚
      â”‚ 1. Update F â† least squares (centroid step)            â”‚
      â”‚ 2. Update W â† Procrustes via SVD                       â”‚
      â”‚ 3. Update G â† k-means assignment in embedded space     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                            ðŸ”š Final Output:
                            - Clustered node labels G
                            - Embedding space Z = Táµ–XW

```
##  LMGEC â€” Linear Multi-view Graph Embedding and Clustering

```text
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚    INPUT MULTI-VIEW GRAPH DATA       â”‚
                     â”‚  V views: {(A_v, X_v)}               â”‚
                     â”‚  - A_v âˆˆ â„â¿Ë£â¿: adjacency matrix       â”‚
                     â”‚  - X_v âˆˆ â„â¿Ë£áµˆáµ›: feature matrix        â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ PER-VIEW PROPAGATION                                 â”‚
          â”‚ H_v = S_v X_v,  S_v = (D_v + Î²I)â»Â¹ (A_v + Î²I)         â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ PER-VIEW EMBEDDING                                    â”‚
       â”‚ W_v âˆˆ â„áµˆáµ›Ë£á¶ ,   Z_v = H_v W_v                           â”‚
       â”‚ Constraint: W_v W_váµ— = I                               â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ GLOBAL CLUSTERING                                           â”‚
   â”‚ - Shared cluster assignments G âˆˆ {0,1}â¿Ë£áµ                   â”‚
   â”‚ - Shared centroids F âˆˆ â„áµË£á¶                                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ATTENTION WEIGHTS OVER VIEWS                                 â”‚
   â”‚ Î±_v = softmax(-I_v / Ï„),  I_v = â€–H_v - G_v F_vâ€–              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ JOINT OBJECTIVE FUNCTION                                     â”‚
   â”‚ L = Î£_v Î±_v â€–H_v - G F W_váµ—â€–Â²                                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ OPTIMIZATION (BLOCK COORDINATE DESCENT)                       â”‚
  â”‚ 1. Update W_v â† SVD(H_váµ— G F) â†’ W_v = VUáµ—                     â”‚
  â”‚ 2. Update F â† Least squares on Î±-weighted sum of views       â”‚
  â”‚ 3. Update G â† Assign via argmin of distance to Î±-weighted F  â”‚
  â”‚ 4. Re-estimate Î±_v using updated reconstruction errors       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                            ðŸ”š Final Output:
                            - Shared cluster labels G
                            - Per-view projections {W_v}
                            - Consensus centroids F

```
## ðŸ” LMGEC Execution Pipeline (Code-Level Overview)
``` text
                      ðŸ”½ ENTRY POINT : Evaluation script
   mvcluster/benchmark/lmgec_benchmark.py
   â””â”€â”€ run_lmgec_experiment(dataset, ...)
        â”‚
        â–¼
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ðŸ“¦ Step 1: Load Dataset

  datagen(dataset)                [utils/datagen.py]
  â”œâ”€â”€ acm(), dblp(), imdb(), etc.
  â””â”€â”€ Returns: As, Xs, labels

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ðŸ“¦ Step 2: Preprocess Views

  preprocess_dataset(A, X, tf_idf, beta)  [utils/preprocess.py]
  â”œâ”€â”€ Normalize adjacency with Î²
  â”œâ”€â”€ Normalize features (L2 or TF-IDF)
  â””â”€â”€ Returns: adj_norm, features_norm

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ðŸ“¦ Step 3: Graph Propagation

  H_v = S @ X                                 [lmgec_benchmark.py]
  H_v = StandardScaler().fit_transform(H_v)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ðŸ“¦ Step 4: Train the Model

  model = LMGEC(...)                        [cluster/lmgec.py]
  model.fit(Hs)
     â”œâ”€â”€ init_W(X, f)                      [utils/init_utils.py]
     â”œâ”€â”€ init_G_F(XW, k)                   [utils/init_utils.py]
     â””â”€â”€ train_loop(...)                   [models/lmgec_core.py]
         â”œâ”€â”€ update_rule_W / G / F
         â””â”€â”€ Î±_v computed via inertia

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ðŸ“¦ Step 5: Predict + Embedding

  model.predict(X_views) â†’ cluster labels
  model.transform(X_views) â†’ projected embeddings

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ðŸ“¦ Step 6: Evaluate

  clustering_accuracy(...)        [utils/metrics.py]
  clustering_f1_score(...)        [utils/metrics.py]
  normalized_mutual_info_score   [sklearn]
  adjusted_rand_score            [sklearn]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ðŸ“¦ Final Results

  Print: ACC, F1, NMI, ARI scores (averaged across runs)
```



# LMGEC Model: Mathematical Formulation and Weight Update Pseudocode

## 1. Mathematical Formulation

### Notations
- \( V \): Number of views, indexed by \( i = 1, \ldots, V \)
- For each view \( i \):
  - \( \mathbf{S}_i \in \mathbb{R}^{n \times n} \): normalized adjacency matrix
  - \( \mathbf{X}_i \in \mathbb{R}^{n \times d_i} \): feature matrix
- \( n \): number of samples
- Parameters:
  - \( T > 0 \): temperature controlling view weighting sharpness
  - \( \beta > 0 \): graph regularization parameter
  - \( k \): number of clusters (embedding dimension is \( k+1 \))

### View Preprocessing
\[
(\mathbf{S}_i, \mathbf{X}_i) \leftarrow \text{preprocess\_dataset}(\mathbf{A}_i, \mathbf{X}_i, \beta)
\]

\[
\mathbf{H}_i = \text{StandardScaler}(\mathbf{S}_i \mathbf{X}_i)
\]


### Score Computation per View
\[
s_i = \|\mathbf{X}_i \mathbf{W} \mathbf{v}\|_F
\]
where \( \mathbf{W}, \mathbf{v} \) are model parameters and \(\|\cdot\|_F\) is Frobenius norm.

### Weight Computation (Softmax with Temperature)
\[
\alpha_i = \frac{\exp(s_i / T)}{\sum_{j=1}^V \exp(s_j / T)}
\]

- \( T \to 0 \): weights become one-hot (selecting one view)
- \( T \to \infty \): weights become uniform

### Fusion of Views
\[
\mathbf{Z} = \sum_{i=1}^V \alpha_i \mathbf{H}_i
\]

### Objective Function
\[
\mathcal{L} = \text{ClusteringLoss}(\mathbf{Z}) + \beta \cdot \text{GraphRegularization}(\{\mathbf{S}_i\})
\]

---

## 2. Weight Update Pseudocode

```python
initialize model parameters W, v, ...

for iteration in range(max_iter):
    scores = []
    for i in range(V):
        Z_i = X_i @ W @ v           # Project view i
        s_i = norm(Z_i, 'fro')      # Compute score for view i
        scores.append(s_i)
    
    # Compute weights alpha with softmax temperature T
    exp_scores = [exp(s / T) for s in scores]
    sum_exp = sum(exp_scores)
    alphas = [e / sum_exp for e in exp_scores]
    
    # Fuse views with weights
    Z = sum(alphas[i] * H_i for i in range(V))
    
    # Update model parameters (e.g., gradient step)
    update_parameters()
    
    if convergence_criteria_met:
        break
```